{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ca0236",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div align=\"center\">\n",
    "<font size=\"6\"><b>ICS2205- Web Group Assignment Assigment</b></font><br><br>\n",
    "<font size=\"5\">Task 2: Text Analysis</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2424b2",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Only run this notebook after trying to run \"Task 2 Full dataset\" and failing at the TF.IDF</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be28cd28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\user\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (4.11.3)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (8.0.4)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flask) (2.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click>=8.0->flask) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->flask) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata->flask) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask flask==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b293b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524b525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords #List of stopwords\n",
    "from nltk.stem import PorterStemmer #Stemming of words\n",
    "#Other\n",
    "import pandas as pd #Used to display TF.IDF\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3bf959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabc376",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>A. Process the news headline text</b></font> <br><br>\n",
    "\n",
    "<font size =\"4\"><b>i. Parse JSON Files</b></font><br>\n",
    "Each line in the provided dataset, is a JSON Object. By using the json package of python, each line in the data set is loaded and then added to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527bb3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_file_path = \"NewsCategoryDataset_2017_2022.json\"\n",
    "limit = 10000\n",
    "data = {}\n",
    "i = 0\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    for line in f:\n",
    "        if(line != '\\n'):\n",
    "            data[i] = json.loads(line)\n",
    "            i += 1\n",
    "        if(i==limit):\n",
    "            break\n",
    "    f.close()\n",
    "# print(data[10][\"headline\"])\n",
    "# print(data[10][\"short_description\"])\n",
    "# print(data[10][\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85dce",
   "metadata": {},
   "source": [
    "For analysis only the headline, category and short_description are required. Thus the following data is saved in its respective dictionaries. The \"data'' dictionary is cleared to free up memory space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcdd473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headlineData = {} #Will store all headline data\n",
    "categoryData = {} #Will store all category data\n",
    "\n",
    "for i in range(len(data)):\n",
    "    headlineData[i] = data[i][\"headline\"] + \" \" + data[i][\"short_description\"] #Stores the headline data\n",
    "    categoryData[i] = data[i][\"category\"] #Stores the categories\n",
    "\n",
    "data = {} #Free up Memory \n",
    "\n",
    "# print(headlineData[10])\n",
    "# print(categoryData[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1e69f",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>ii. Lexical Analyses</b></font><br>\n",
    "Currently all the text found in headline data is in the form of a string. To be able to produce a TF.IDF matrix the text needs to be separated into strings containing a single word. This process is done using the “re” package [1]. Essentially for each of the headlines, the string is divided at each character that is not part of the alphabet.<br><br>\n",
    "Case folding is also being performed at the same time to convert each of the characters in the string to lowercase. This is needed for the TF.IDF matrix to prevent having multiple distinct words representing the same word just with different casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b125ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(headlineData)): \n",
    "    token = [] # Will hold list of words\n",
    "    tempString = headlineData[i]\n",
    "    tokens = re.findall(r\"[\\w']+\", tempString.casefold()) #Will tokenize the data by spaces and punct\n",
    "    headlineData[i] = tokens\n",
    "# print(headlineData[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e669e43",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>iii. Stop Words & iv. Porter's Stemmer</b></font><br>\n",
    "The headline data still currently contains a lot of commonly used words which are also referred to as stop words. These words are removed by comparing them with a list of stop words from the “NLTK” library [2]. If any words in the headline matches the stop words list the word is removed from the headline.<br><br>\n",
    "Some words have similar meanings or are variations of a word which is their stem. Using the NLTK's library Porter's Stemmer [3] each word in the headline is reduced to its stem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7e874d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer() # Using NLTK library \n",
    "stop_words = set(stopwords.words('english')) #Using NLTK library\n",
    "\n",
    "for i in range(len(headlineData)):\n",
    "    filtered_sentence = []\n",
    "    for w in headlineData[i]:\n",
    "        if w not in stop_words: # Check word is not a stop word\n",
    "            filtered_sentence.append(ps.stem(w)) # Reduce a word to its stem\n",
    "    headlineData[i] = filtered_sentence\n",
    "# print(headlineData[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10db034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of articles:  10000\n"
     ]
    }
   ],
   "source": [
    "headlineAmount = len(headlineData)\n",
    "print(\"Amount of articles: \", headlineAmount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513875a8",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b> B. Construct a TF.IDF</b></font> <br><br>\n",
    "<font size =\"4\"><b>Calculate Unique Words</b></font><br>\n",
    "The distinct words in that form part of the headline data are found and put in a set. These words are going to be used for the TF.IDF and for extracting the data to JSON. There are a total of 36,120 distinct words in the headlines and short descriptions combined together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3faec0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18496\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = set() #Holds every word present in the data\n",
    "for i in range(len(headlineData)): #Loop through words\n",
    "    for word in headlineData[i]:\n",
    "        if word not in uniqueWords:\n",
    "            uniqueWords.add(str(word))\n",
    "\n",
    "print(len(uniqueWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0787bd",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>Term Frequency</b></font><br>\n",
    "The term frequency is the frequency of a word in a single document with regards to the document’s length. This is obtained by counting each occurance of the word, then dividing the number of occurrences by the number of words in the document. [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d3d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TermFrequency(doc,word):\n",
    "    docLength = len(doc)\n",
    "    \n",
    "    occurance = 0\n",
    "    for w in doc:\n",
    "        if w == word:\n",
    "            occurance += 1\n",
    "\n",
    "    return occurance / docLength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf20d3a",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>Inverse Document Frequency</b></font><br>\n",
    "The inverse document frequency(IDF) determines the frequency of a word throughout documents in a dataset. IDF is used to aid the score of words that are unique to specific documents. [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc9f347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountDict(headlineData):\n",
    "    \n",
    "    countDict = {} #Will hold the number of times a word appears in distinct documents. Using the unique words a key and\n",
    "                    #the number of times a word appears as the value for said key\n",
    "        \n",
    "    #Initiliaze all keys to 0 to avoid error and for simplicity\n",
    "    for word in uniqueWords:\n",
    "        countDict[word] = 0\n",
    "        \n",
    "    #Loop through all the words\n",
    "    for i in range(headlineAmount):\n",
    "        for word in headlineData[i]:\n",
    "            usedWords = set() #usedWords will hold the words that have already be incremented for a specific document\n",
    "                              #Its usage is to prevent increment the frequency for the same word in the same doc twice. \n",
    "            for word in headlineData[i]:\n",
    "                if word not in usedWords:\n",
    "                    usedWords.add(word)\n",
    "                    countDict[word] += 1 #Increments frequency\n",
    "    return countDict\n",
    "\n",
    "wordFreq = CountDict(headlineData)\n",
    "#print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4482f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word):\n",
    "    try:\n",
    "        word_occurance = wordFreq[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(headlineAmount / word_occurance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191698ab",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>TF.IDF</b></font><br>\n",
    "TF.IDF is used to search documents and extract the most important terms in relation to a specific document. The highest scoring words are considered the keywords for a specific document. [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c1275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc):\n",
    "    vec = np.zeros((len(uniqueWords),))\n",
    "    for word in doc:\n",
    "        tf = TermFrequency(doc, word)\n",
    "        idf = inverse_document_frequency(str(word))\n",
    "        vec[uniqueWords_index[word]] = tf * idf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3b1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords_index = {}\n",
    "for i, word in enumerate(uniqueWords):\n",
    "    uniqueWords_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27dd3bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TF_IDF = []\n",
    "for i in range(headlineAmount):\n",
    "    TF_IDF.append(tf_idf(headlineData[i]))\n",
    "# print(TF_IDF[10][uniqueWords_index[\"qatar\"]])\n",
    "# print(len(TF_IDF))\n",
    "# print(len(uniqueWords_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb3b54",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b> Displaying the TF.IDF</b></font><br>\n",
    "The TF.IDF is displayed using the panda dataframe. The tf idf is limited to the first 10 articles to be memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580f69fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article1</th>\n",
       "      <th>article2</th>\n",
       "      <th>article3</th>\n",
       "      <th>article4</th>\n",
       "      <th>article5</th>\n",
       "      <th>article6</th>\n",
       "      <th>article7</th>\n",
       "      <th>article8</th>\n",
       "      <th>article9</th>\n",
       "      <th>article10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368247</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sept</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273951</td>\n",
       "      <td>0.16188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaten</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403081</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pix11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isabel</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>-0.013193</td>\n",
       "      <td>-0.014914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>-0.040892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.039377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>-0.027910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.038193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18496 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article1  article2  article3  article4  article5  article6  article7  \\\n",
       "23      0.000000  0.000000  0.368247   0.10880       0.0  0.000000  0.000000   \n",
       "sept    0.000000  0.000000  0.273951   0.16188       0.0  0.000000  0.000000   \n",
       "eaten   0.000000  0.000000  0.403081   0.00000       0.0  0.000000  0.000000   \n",
       "pix11   0.000000  0.000000  0.000000   0.00000       0.0  0.000000  0.327085   \n",
       "isabel  0.000000  0.000000  0.000000   0.00000       0.0  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "say     0.000000  0.000000  0.000000   0.00000       0.0  0.000000  0.000000   \n",
       "year    0.000000  0.000000  0.000000   0.00000       0.0 -0.018032  0.000000   \n",
       "u      -0.013193 -0.014914  0.000000   0.00000       0.0  0.000000  0.000000   \n",
       "said   -0.040892  0.000000  0.000000   0.00000       0.0 -0.039377  0.000000   \n",
       "new    -0.027910  0.000000  0.000000   0.00000       0.0  0.000000 -0.038193   \n",
       "\n",
       "        article8  article9  article10  \n",
       "23      0.000000  0.000000   0.000000  \n",
       "sept    0.000000  0.000000   0.000000  \n",
       "eaten   0.000000  0.000000   0.000000  \n",
       "pix11   0.000000  0.000000   0.000000  \n",
       "isabel  0.000000  0.308291   0.000000  \n",
       "...          ...       ...        ...  \n",
       "say     0.000000  0.000000  -0.030854  \n",
       "year    0.000000  0.000000  -0.020286  \n",
       "u      -0.017151  0.000000  -0.014293  \n",
       "said    0.000000  0.000000   0.000000  \n",
       "new     0.000000 -0.036283   0.000000  \n",
       "\n",
       "[18496 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf2 = pd.DataFrame(data = TF_IDF[:10])\n",
    "tf_idf2.columns = uniqueWords_index\n",
    "tfidf_matrix = tf_idf2.T\n",
    "tfidf_matrix.columns = ['article'+ str(i) for i in range(1, 10+1)]\n",
    "tfidf_matrix['count'] = tfidf_matrix.sum(axis=1)\n",
    "tfidf_matrix = tfidf_matrix.sort_values(by ='count', ascending=False)\n",
    "display(tfidf_matrix.drop(columns=['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf3ad4",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>C. Extract Category Information</b></font> <br><br>\n",
    "<font size =\"4\"><b>Calculate Unique Categories</b></font><br>\n",
    "The unique categories are added to a set()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc772c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniqueCategory = set() #Hold all the uniqueCategories \n",
    "for i in range(len(categoryData)):\n",
    "    if categoryData[i] not in uniqueCategory:\n",
    "        uniqueCategory.add(categoryData[i])\n",
    "# print(len(uniqueCategory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1649ce",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>Group together headlines in the same category</b></font><br>\n",
    "Using a dictionary, the headlines of a specific category are grouped. The index of the headline instead of the headline itself is stored to preserve memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82414e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryHeadlines = {}  #Will hold all headlines index related to specific category\n",
    "                        #Uses cateragory as a key, hold headlines indexes\n",
    "\n",
    "for category in uniqueCategory: #Init Dictionary with empty list\n",
    "    categoryHeadlines[category] = [] \n",
    "\n",
    "for i in range(headlineAmount): #Loop through all headlines and append them to relevant category\n",
    "    categoryHeadlines[categoryData[i]].append(i)\n",
    "    \n",
    "#print(categoryHeadlines[\"SPORTS\"][:50]) #Prints the first 50 headlines sports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769b6ee",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>i. Calculate the average weighting of a word</b></font><br>\n",
    "Using the constructed TF.IDF, the average weighting of each word in a category is calculated. The weights are stored in a nested dictionary . The dictionary will have its parent key be the category and the sub-dictionary will have the words as keys and the value as scores.\n",
    "<br><br>\n",
    "\n",
    "<font size =\"4\"><b>ii. Get the highest-weighted n% of the terms for each category.</b></font><br>\n",
    "For each category the only terms that are kept are the following: <br>\n",
    "1. If the amount of terms is less than 100, all terms are kept. N = Amount of terms\n",
    "1. If the amount of terms is between 100 and 1000, the top 100 terms are kept. N = 100\n",
    "1. If the amount of terms is greater than 1000, the top 10% of terms are kept. N% = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cc1ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryDictionary = {} #Will hold nested Dictionaries\n",
    "\n",
    "for category in categoryHeadlines:#For each category\n",
    "    tempDict = {} #Tempory dictionary to hold weighting of words\n",
    "    \n",
    "    for word in uniqueWords:\n",
    "        tempDict[word] = 0 #Initialize keys\n",
    "        \n",
    "    for index in categoryHeadlines[category]: #For each headline in each category\n",
    "        for word in headlineData[index]:\n",
    "            tempDict[word] += TF_IDF[index][uniqueWords_index[word]] #Add the word importance for that document\n",
    "    \n",
    "    for word in uniqueWords:\n",
    "        if tempDict[word] == 0: \n",
    "            tempDict.pop(word) #Remove any word that where not used in a specific category\n",
    "        else:\n",
    "            tempDict[word] = tempDict[word]/len(categoryHeadlines[category]) #Normalise to avoid large scores in larger categories\n",
    "            tempDict[word] *= 100 #Uniformal Increae to make score differences more noticable\n",
    "    \n",
    "    #ii. Get the highest-weighted n% of the terms for each category.\n",
    "    \n",
    "    #Reduce to N% of values\n",
    "    sortedDict = sorted(tempDict.items(), key=lambda x:x[1], reverse = True) # Sort category\n",
    "    termAmount = len(sortedDict)\n",
    "    \n",
    "    #If Terms <= 100 do nothing\n",
    "\n",
    "    #If Terms > 100 & less < 1000  reduce to top 100\n",
    "    if(termAmount > 100) and (termAmount < 1000):\n",
    "        sortedDict = sortedDict [:100] #Reducing to the top 100\n",
    "    w\n",
    "    #If Terms > 1000 reduce to top 10%\n",
    "    if(termAmount > 1000):\n",
    "        nTerms = round(0.1 * termAmount)\n",
    "        sortedDict = sortedDict [:nTerms]\n",
    "            \n",
    "    categoryDictionary[category] = dict(sortedDict)\n",
    "# print(categoryDictionary[\"SPORTS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae0e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to correct format for keyword cloud\n",
    "\n",
    "categoryDictionary2 = {}\n",
    "for key in categoryDictionary:\n",
    "    categoryDictionary2[key] = [] #List of dictionaries\n",
    "    for nestedKey in categoryDictionary[key]:\n",
    "        categoryDictionary2[key].append({\"text\": nestedKey, \"size\": categoryDictionary[key][nestedKey]}) #Keyword Format\n",
    "    \n",
    "categoryDictionary = categoryDictionary2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fda09",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>iii. Extracting all the data into JSON files</b></font><br>\n",
    "\n",
    "Re-open the data files containing all the data and store data in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49706b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "i = 0\n",
    "with open(json_file_path) as f:\n",
    "    f.seek(0)\n",
    "    for line in f:\n",
    "        if(line != '\\n'):\n",
    "            data[i] = json.loads(line)\n",
    "            i += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33f55d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorizedData = {} # Getting the headlines data instead of indexes in the dictionary\n",
    "for category in uniqueCategory:\n",
    "    headlinesIndexes = categoryHeadlines[category]\n",
    "    categorizedData[category] = {}\n",
    "    for index in headlinesIndexes:\n",
    "        categorizedData[category][index] = data[index] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97772a2",
   "metadata": {},
   "source": [
    "categoryJsonData = { } will hold all the article data grouped by categories and has the following structure:\n",
    "* Category\n",
    "    * listOfArticles\n",
    "        * (Article Data)\n",
    "    * top_terms\n",
    "        * text\n",
    "        * size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd510ab",
   "metadata": {},
   "source": [
    "categoryBubbleJsonData = [ ] will hold the data neccassary to construct the bubble chart for the bubble graph and contains a list of dictionaries of the following structure:\n",
    "* name (The name of category)\n",
    "* score (Number of articles in the category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219549ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryJsonData = {}\n",
    "categoryBubbleJsonData = []\n",
    "#categoryJsonData[category][list]/top term\n",
    "for category in uniqueCategory:\n",
    "    categoryJsonData[category] = {}\n",
    "\n",
    "for category in uniqueCategory:\n",
    "    categoryJsonData[category][\"listOfArticles\"] = categorizedData[category]\n",
    "    categoryJsonData[category][\"top_terms\"] = categoryDictionary[category]\n",
    "\n",
    "\n",
    "for category in uniqueCategory:\n",
    "    tempDic = {}\n",
    "    tempDic[\"name\"] = category \n",
    "    tempDic[\"score\"] = len(categoryJsonData[category][\"listOfArticles\"])\n",
    "    categoryBubbleJsonData.append(tempDic)\n",
    "    \n",
    "# print(categoryBubbleJsonData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c177a",
   "metadata": {},
   "source": [
    "Using the json library, the dictionaries are writen to their respective files\n",
    "* categoryData.json\n",
    "* categoryBubbleData.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ad50eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"categoryData.json\",\"w\") as outfile:\n",
    "    json.dump(categoryJsonData,outfile)\n",
    "outfile.close()\n",
    "\n",
    "with open(\"static/categoryBubbleData.json\",\"w\") as outfile:\n",
    "       json.dump(categoryBubbleJsonData,outfile)\n",
    "outfile.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91ddbe",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>D. K-means</b></font> <br><br>\n",
    "Generate a set of random cluster nodes to start clustering. The number of nodes (k) = number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef3dfa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNodes = [] #Contains Headline Indexes\n",
    "for i in range(len(uniqueCategory)):\n",
    "    randomNum = random.randint(0,headlineAmount)\n",
    "    if(randomNum not in clusterNodes):\n",
    "        clusterNodes.append(randomNum)\n",
    "    else:\n",
    "        i -= 1\n",
    "# print(clusterNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7647202",
   "metadata": {},
   "source": [
    "<b>Cosine Similarity Function</b>: Used to compare the similarity between two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1f3adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(node, Y_set):\n",
    "    c = 0\n",
    "    l1 =[]\n",
    "    l2 =[]\n",
    "    X_set = {}\n",
    "    rvector = []\n",
    "\n",
    "    X_set = {w for w in headlineData[node]} #Grab Node\n",
    "    rvector = X_set.union(Y_set) #Union Them\n",
    "\n",
    "    for w in rvector:\n",
    "        if w in X_set: \n",
    "            l1.append(TF_IDF[node][uniqueWords_index[w]]) #Value of Wieghting for that document\n",
    "        else: \n",
    "            l1.append(0)\n",
    "\n",
    "        if w in Y_set:\n",
    "            l2.append(1)\n",
    "        else: \n",
    "            l2.append(0)\n",
    "\n",
    "    # cosine formula \n",
    "    for j in range(len(rvector)):\n",
    "        c+= l1[j]*l2[j]\n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f809f41",
   "metadata": {},
   "source": [
    "<b>kMeans Function</b>: Clusters the headline data into clusters. It will iterate through every headline for each node. While iterating it will check the cosine similarity of the headline with the current node and store for each headline, the node it's the most similiar to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d71ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kMeansClustering(startingNodes,maxIt):\n",
    "    change = True # Flags if a change occured in the nodes between one iteration and the next\n",
    "    iterations = 0 # The number of current iteration \n",
    "    nodeAmount = len(clusterNodes) # The number of clusters\n",
    "    \n",
    "    while (change == True) and (iterations < maxIt): # If there was a change and max iterations not reach\n",
    "        kMeansDic = {} \n",
    "        totalCosineDic = {}\n",
    "        meanCosineDic = {}\n",
    "        index = 0\n",
    "        testing = 0\n",
    "        \n",
    "        for nd in clusterNodes: #For every k, create and empty list\n",
    "            kMeansDic[nd] = []\n",
    "            totalCosineDic[nd] = 0 #Initialize co-sine similarity to 0\n",
    "            \n",
    "        for headline in headlineData:\n",
    "            Y_set = {w for w in headlineData[headline]} #Query == Entire headline\n",
    "            sim = [] \n",
    "            \n",
    "            bestCosine = 0 #Best Cosine Similirity\n",
    "            bestNode = -10 #Keeps track which node the document is closest too\n",
    "            totalCosine = 0 #Reset the total Cosine\n",
    "            \n",
    "            for node in clusterNodes:\n",
    "                cosine = cosineSimilarity(node, Y_set) # Get Cosine Similiarity\n",
    "            \n",
    "                if(cosine > bestCosine): #Store best Cosine\n",
    "                    bestNode = node\n",
    "                    bestCosine = cosine\n",
    "        \n",
    "            #Catches Nodes that don't get assigned a cluster\n",
    "            if (bestNode == -10):\n",
    "                randomNum = random.randint(0,nodeAmount-1)\n",
    "                kMeansDic[clusterNodes[randomNum]].append((index,bestCosine))\n",
    "                testing += 1\n",
    "            else:\n",
    "                kMeansDic[bestNode].append((index,bestCosine))\n",
    "                totalCosineDic[bestNode] += bestCosine\n",
    "            index += 1\n",
    "        \n",
    "        iterations += 1\n",
    "        if(iterations != maxIt): #Unless Limit not reach\n",
    "            change = False\n",
    "            for j in range(len(clusterNodes)): #For each starting node\n",
    "                node = clusterNodes[j] \n",
    "                total = totalCosineDic[node]\n",
    "                length = len(kMeansDic[node])\n",
    "                mean = total/length # Calculate mean of new node\n",
    "                \n",
    "                bestFit = 100\n",
    "                bestIndex = node\n",
    "                \n",
    "                for tup in kMeansDic[node]: #Search to find new node with closest similarity\n",
    "                    diff = abs(tup[1]-mean) #Difference with the cosine similarity of a headline and the mean\n",
    "                    \n",
    "                    if(diff < bestFit): \n",
    "                        bestFit = tup[1]\n",
    "                        bestIndex = tup[0]\n",
    "                        \n",
    "                if(node != bestIndex): #If a change occurred\n",
    "                    clusterNodes[j] = bestIndex\n",
    "                    change = True\n",
    "        else:\n",
    "            return kMeansDic\n",
    "maxIt = 10\n",
    "kMeansDic = kMeansClustering(clusterNodes,maxIt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e128f2",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>E. Extract K-Means Information</b></font><br><br>\n",
    "<font size =\"4\"><b>i. Calculate the average weighting of a word</b></font><br>\n",
    "Using the constructed TF.IDF, the average weighting of each word in a cluster is calculated. The weights are stored in the a nested dictionary. The dictionary will have its parent key be the cluster node and the sub-dictionary will be have the words as keys and the value as scores.<br><br>\n",
    "\n",
    "<font size =\"4\"><b>ii. Get the highest-weighted n% of the terms for each category.</b></font><br>\n",
    "For each cluster the only terms that are kept are the following: <br>\n",
    "1. If the amount of terms is less than 100, all terms are kept. N = Amount of terms\n",
    "1. If the amount of terms is between 100 and 1000, the top 100 terms are kept. N = 100\n",
    "1. If the amount of terms is greater than 1000, the top 10% of terms are kept. N% = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansDictionary = {} #Will hold nested Dictionaries\n",
    "\n",
    "for node in kMeansDic:#For each category\n",
    "    tempDict = {} #Tempory dictionary to hold weighting of words\n",
    "    \n",
    "    for word in uniqueWords:\n",
    "        tempDict[word] = 0 #Initialize keys\n",
    "        \n",
    "    for tup in kMeansDic[node]: #For each headline in each cluster\n",
    "        index = tup[0]\n",
    "        for word in headlineData[index]:\n",
    "            tempDict[word] += TF_IDF[index][uniqueWords_index[word]] #Add the word importance for that document\n",
    "    \n",
    "    for word in uniqueWords:\n",
    "        if tempDict[word] == 0: \n",
    "            tempDict.pop(word) #Remove any word that where not used in a specific cluster\n",
    "        else:\n",
    "            tempDict[word] = tempDict[word]/len(kMeansDic[node]) #Normalise to avoid large scores in larger categories\n",
    "            tempDict[word] *= 100 #Uniformal Increae to make score differences more noticable\n",
    "    \n",
    "    #ii. Get the highest-weighted n% of the terms for each category.\n",
    "    \n",
    "    #Reduce to N% of values\n",
    "    sortedDict = sorted(tempDict.items(), key=lambda x:x[1], reverse = True) # Sort category\n",
    "    termAmount = len(sortedDict)\n",
    "    \n",
    "    #If Terms <= 100 do nothing\n",
    "\n",
    "    #If Terms > 100 & less < 1000  reduce to top 100\n",
    "    if(termAmount > 100) and (termAmount < 1000):\n",
    "        sortedDict = sortedDict [:100] #Reducing to the top 100\n",
    "    w\n",
    "    #If Terms > 1000 reduce to top 10%\n",
    "    if(termAmount > 1000):\n",
    "        nTerms = round(0.1 * termAmount)\n",
    "        sortedDict = sortedDict [:nTerms]\n",
    "            \n",
    "    KMeansDictionary[node] = dict(sortedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KMeansDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f148f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to correct format for keyword cloud\n",
    "\n",
    "KMeansDictionary2 = {}\n",
    "for key in KMeansDictionary:\n",
    "    KMeansDictionary2[key] = [] #List of dictionaries\n",
    "    for nestedKey in KMeansDictionary[key]:\n",
    "        KMeansDictionary2[key].append({\"text\": nestedKey, \"size\": KMeansDictionary[key][nestedKey]}) #Keyword Format\n",
    "\n",
    "KMeansDictionary = KMeansDictionary2\n",
    "clusterDictionary2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33db14",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b>iii. Extracting all the data into JSON files</b></font><br>\n",
    "\n",
    "Re-open the data files containing all the data and store data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "i = 0\n",
    "with open(json_file_path) as f:\n",
    "    f.seek(0)\n",
    "    for line in f:\n",
    "        if(line != '\\n'):\n",
    "            data[i] = json.loads(line)\n",
    "            i += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterData = {} # Getting the headlines data instead of indexes in the dictionary\n",
    "for node in KMeansDictionary:\n",
    "\n",
    "    headlinesIndexes = kMeansDic[node]\n",
    "    clusterData[node] = {}\n",
    "    for tup in headlinesIndexes:\n",
    "        index = tup[0]\n",
    "        clusterData[node][index] = data[index] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412dc47",
   "metadata": {},
   "source": [
    "clusterJsonData = { } will hold all the headline data grouped by cluster nodes and has the following structure:\n",
    "* ClusterNode\n",
    "    * listOfArticles\n",
    "        * (Article Data)\n",
    "    * top_terms\n",
    "        * text\n",
    "        * size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00654d6",
   "metadata": {},
   "source": [
    "clusterBubbleJsonData = [ ] will hold the data neccassary to construct the bubble chart for the bubble graph and contains a list of dictionaries of the following structure:\n",
    "* name (The index of the document currently being a centeriod)\n",
    "* score (Number of articles in the cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06665bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterJsonData = {}\n",
    "clusterBubbleJsonData = []\n",
    "#categoryJsonData[category][list]/top term\n",
    "for node in KMeansDictionary:\n",
    "    clusterJsonData[node] = {}\n",
    "\n",
    "for node in KMeansDictionary:\n",
    "    clusterJsonData[node][\"listOfArticles\"] = clusterData[node]\n",
    "    clusterJsonData[node][\"top_terms\"] = KMeansDictionary[node]\n",
    "\n",
    "for node in KMeansDictionary:\n",
    "    tempDic = {}\n",
    "    tempDic[\"name\"] = str(node)\n",
    "    tempDic[\"score\"] = len(clusterJsonData[node][\"listOfArticles\"])\n",
    "    clusterBubbleJsonData.append(tempDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clusterData.json\",\"w\") as outfile:\n",
    "    json.dump(clusterJsonData,outfile)\n",
    "outfile.close()\n",
    "\n",
    "with open(\"static/clusterBubbleData.json\",\"w\") as outfile:\n",
    "       json.dump(clusterBubbleJsonData,outfile)\n",
    "outfile.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5bbdb",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<font size=\"5\">Task 2: Web Application</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8f630",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>A. Set up Flask</b></font> <br><br>\n",
    "<font size =\"4\"><b>ii. Import Json Files</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"NewsCategoryDataset_2017_2022.json\" #Raw JSON File\n",
    "\n",
    "data = {}\n",
    "i = 0\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    for line in f:\n",
    "        if(line != '\\n'):\n",
    "            data[i] = json.loads(line)\n",
    "            i += 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"categoryData.json\" #Category Data JSON File\n",
    "\n",
    "categData = {}\n",
    "i = 0\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    categData = json.load(f)\n",
    "    f.close()\n",
    "# print(categData[\"SPORTS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b93552",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"clusterData.json\" #Cluster Data JSON File\n",
    "\n",
    "clustData = {}\n",
    "i = 0\n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    clustData = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html',data = data)\n",
    "\n",
    "@app.route(\"/<int:index>\")\n",
    "def article(index):\n",
    "    index = int(index)\n",
    "    return render_template('article.html', art_id = index, headline = data[index][\"headline\"],\n",
    "                           short_description = data[index][\"short_description\"],date = data[index][\"date\"],\n",
    "                           link = data[index][\"link\"])\n",
    "@app.route('/category')\n",
    "def category():\n",
    "    return render_template('category.html')\n",
    "\n",
    "@app.route('/cluster')\n",
    "def cluster():\n",
    "    return render_template('cluster.html')\n",
    "\n",
    "@app.route('/cluster/<clust>')\n",
    "def clust(clust):\n",
    "    keywordData = clustData[clust][\"top_terms\"]\n",
    "    return render_template('clust.html', data = clustData[clust], keywordData = keywordData)\n",
    "\n",
    "@app.route('/category/<categ>')\n",
    "def categ(categ):\n",
    "    categ = categ.upper()\n",
    "    keywordData = categData[categ][\"top_terms\"]\n",
    "    return render_template('categ.html', data = categData[categ], keywordData = keywordData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412c4c3",
   "metadata": {},
   "source": [
    "<font size =\"4\"><b><font size =\"4\"><b>iii. Show the list of documents as a ‘clickable’ list</b></font> </b></font><br>\n",
    "This is the home page (index.html) of Flask server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23c630",
   "metadata": {},
   "source": [
    "<img src= \"images/All Articles.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea2f73",
   "metadata": {},
   "source": [
    "<div align=center><b>Figure 1:</b> List of clickable headlines</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5926e",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>B. Show headline information</b></font> <br>\n",
    "\n",
    "<b>Instructions:</b>\n",
    "Click on any of the links in figure 1 to open up the headline. The url will indicate the headline you are currently visiting. You can visit a specific url by adding the index number to default url such as: http://127.0.0.1:5000/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36ad4a",
   "metadata": {},
   "source": [
    "<img src= \"images/Headline_Info.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c07a8",
   "metadata": {},
   "source": [
    "<div align=center><b>Figure 2:</b> An example of a headline</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86429239",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>C. Show the list of categories as an interactive bubble chart.</b></font> <br><br>\n",
    "\n",
    "Bubble Chart were constructed using D3.js[7]<br>\n",
    "Keyword Clouds were constructed using Easy Word Cloud using D3[8]<br>\n",
    "\n",
    "<b>Instructions:</b>\n",
    "This page can be reached by adding category to the default url such as this: http://127.0.0.1:5000/category\n",
    "\n",
    "A specific category can be accessed by adding category followed by the name of the category to defualt url like the following: http://127.0.0.1:5000/category/POLITICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cb3c7",
   "metadata": {},
   "source": [
    "<img src= \"images/categoryBubbleChart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc60f26",
   "metadata": {},
   "source": [
    "<div align=center><b>Figure 3:</b> Category Bubblechart</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ed050",
   "metadata": {},
   "source": [
    "<img src= \"images/PoliticsExample.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320a0da",
   "metadata": {},
   "source": [
    "<div align=center><b>Figure 4:</b> Expaning the bubble \"Politics\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d9c9d",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>D. Show the list of clusters as an interactive bubble chart.</b></font> <br><br>\n",
    "<b>Instructions:</b>\n",
    "This page can be reached by adding category to the default url such as this: http://127.0.0.1:5000/cluster\n",
    "\n",
    "A specific category can be accessed by adding category followed by the name of the category to defualt url: http://127.0.0.1:5000/cluster/23856"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25eb49",
   "metadata": {},
   "source": [
    "<img src= \"images/clusterBubbleChart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be2c1d",
   "metadata": {},
   "source": [
    "<div align=center><b>Figure 5:</b> Cluster Bubblechart</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c850c2c",
   "metadata": {},
   "source": [
    "<img src= \"images/23856Example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b2386",
   "metadata": {},
   "source": [
    "<div align=center><b>Figure 6:</b> Expanding the bubble: 23856</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c181f",
   "metadata": {},
   "source": [
    "<font size =\"5\"><b>Reference List</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b963a",
   "metadata": {},
   "source": [
    "[1] re — Regular expression operations, Python. https://docs.python.org/3/library/re.html (accessed 28/01/2023) <br>\n",
    "[2] NLTK stop words, Python. https://pythonspot.com/nltk-stop-words/ (accessed 28/01/2023) <br>\n",
    "[3] Sample usage for stem, NLTK. https://www.nltk.org/howto/stem.html (accessed 28/01/2023) <br>\n",
    "[4] K.Ganesan , What is Term Frequency? Opinosis Analytics. https://www.opinosis-analytics.com/knowledge-base/term-frequency-explained/#:~:text=TF(t)%20%3D%20(Number,of%20terms%20in%20the%20document). (accessed 28/01/2023) <br>\n",
    "[5] K.Ganesan, What is Inverse Document Frequency (IDF)? Kavita Ganesan. https://kavita-ganesan.com/what-is-inverse-document-frequency/#.Y9KclnbMKUk  (accessed 28/01/2023) <br>\n",
    "[6] B.Stecanella, Understanding TF-IDF: A Simple Introduction, Monkey Learn. https://monkeylearn.com/blog/what-is-tf-idf/   (accessed 28/01/2023) <br>\n",
    "[7] F.Almasi, How to Make Interactive Bubble Charts in D3.js, Webtips.  https://www.webtips.dev/how-to-make-interactive-bubble-charts-in-d3-js (accessed by 28/01/2023) <br>\n",
    "[8] wvengen, Easy Word Cloud using D3, Github. https://github.com/wvengen/d3-wordcloud (accessed 28/01/2023) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
